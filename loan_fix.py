# -*- coding: utf-8 -*-
"""loan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jZ5zA46nh6Z_oco9WZrsqPCi1n_3uL5_

# Credit Loan Prediction

This Notebook inspired by [Nadyana](https://www.kaggle.com/nadyana) on kaggle.

This prediction intend to evaluating whether a potential borrower will repay a loan, especially true in peer-to-peer lending, where issues related to class imbalance are widespread.
"""

# import library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import gdown

"""# Data Preparation"""

# Read and load data into dataframe
loan_data = pd.read_csv('loan_data_2007_2014.csv')
loan_data.shape

"""This data have **466285** rows and **75** features that we have to remove whether irrelevant feature"""

# check all the features
loan_data.info()

"""# Data Pre-processing, Cleaning, and Feature Engineering



*   check data (missing value, etc)

Drop columns

*   Drop column 'Unnamed: 0' which is a copy of an index.
*   Drop the columns having > 50% missing values. (columns with 0 unique value are also columns that have 100% missing value)
*   Drop column 'application_type' and 'policy_code' (it only have 1 unique value).
*   Drop identifier columns: id, member_id, title, emp_title, url, zip_code, desc, policy_code (it can not be used in building model).
Drop sub_grade, it contains the same information as the grade columns.


"""

# check the missing data
loan_data.isnull().sum()

"""# Define Target Variable / Labeling"""

# check the value of each loan_status column, loan_status is the target data
loan_data['loan_status'].value_counts()

# check the overall value of loan_status column
loan_data['loan_status'].count()

# we can get the proportion of observations for each unique value of a variable from Dividing the number of observations for each unique value of a variable by the total number of observations
loan_data['loan_status'].value_counts() / loan_data['loan_status'].count()

"""Because we wanted to predict whether a loan is risky or not. Then we need to know the end of each loans historically, wether the loan was defaulted / charged off, or fully paid. Next, we will classy the loan as good loans (non risky) and bad loans (risky).

based on the data above, the loan will classify as follows: '

*   good loans = ['Current', 'Fully Paid', 'In Grace Period', 'Does not meet the credit policy. Status:Fully Paid']
*   bad loans = ['Charged Off', 'Late (31-120 days)', 'Late (16-30 days)', 'Default', 'Does not meet the credit policy. Status:Charged Off']



"""

# define values
good_loans = ['Current', 'Fully Paid', 'In Grace Period',
              'Does not meet the credit policy. Status:Fully Paid']

# create new column to classify ending
loan_data['good_bad_loan'] = np.where(loan_data['loan_status'].isin(good_loans), 1, 0)

loan_data['good_bad_loan']

# check balance
plt.title('Good (1) vs Bad (0) Loans Balance')
sns.barplot(x=loan_data.good_bad_loan.value_counts().index,y=loan_data.good_bad_loan.value_counts().values)

# get a list of columns that have more than 50% null values
na_values = loan_data.isnull().mean()
na_values[na_values>0.5]

# Filtering data with less than 2 unique values
loan_data.nunique()[loan_data.nunique() < 2].sort_values()

# Drop the irrelevant columns
loan_data.drop(['Unnamed: 0', 'desc', 'mths_since_last_delinq', 'mths_since_last_record', 'mths_since_last_major_derog',
                'annual_inc_joint', 'dti_joint', 'verification_status_joint', 'open_acc_6m', 'open_il_6m',
                'open_il_12m', 'open_il_24m','mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m',
                'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl', 'inq_last_12m', 'policy_code',
                'application_type','id', 'member_id', 'sub_grade', 'emp_title', 'url', 'title',
                'zip_code'], axis=1, inplace = True)

"""Data/column understanding is important. We wanted to predict whether a loan is risky or not, before we invest in the loan, not after. The problem with our data is in the columns related to the current status of the loan. We can only get the data of those columns after the loan is issued, in other words, after we invested in the loan.



*   Columns related to the current status of the loan (after it is issued): 'issue_d', 'loan_status', 'pymnt_plan', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', next_pymnt_d

For example, 'out_prncp' (outstanding principal (Remaining outstanding principal for total amount funded)), when out_prncp is 0, then it means the loan is already fully paid, it's easy to predict based on this one variable alone, and it will be super accurate. Another example is with 'recoveries', recoveries only happened after a borrower is unable to repay a loan and the lending institution initiates a loan recovery process. Of course, we know that the loan is bad and risky, just from this info alone. Those variables can predict so accurately because it's already happened.

In data science this kind of variable is called Data Leakage. Data Leakage is the creation of unexpected additional information in the training data, allowing a model or machine learning algorithm to make unrealistically good predictions. This is the data that we won't get when we use the model in deployment. We won't know if there'll be recovery fee, or if the outstanding principal will be 0 or not before the loan is concluded. We won't get any of those data before we invest in the loan.

So, those columns that contain Data Leakage will be drop and only keep the column with data that can be obtained before the loan is invested in.
"""

leakage_col = ['issue_d', 'loan_status', 'pymnt_plan', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv',
                'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',
                'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d']

loan_data.drop(columns=leakage_col, axis=1, inplace=True)

loan_data.info()

"""We have to check the similiar correlation, so we can know which the irrelevant columns"""

#Check correlation
plt.figure(figsize=(24,24))
sns.heatmap(loan_data.corr(), annot=True, annot_kws={'size':14})

"""*loan_amnt, funded_amnt, funded_amnt_inv have similar correlation to other columns. So these columns probably has almost similar data"""

# Check the suspect similar columns
loan_data[['loan_amnt','funded_amnt','funded_amnt_inv']].describe()

# based on the output, the data is so similar, and we can remove 2 of them.
loan_data.drop(columns = ['funded_amnt', 'funded_amnt_inv'], inplace = True)

#Checking for missing values
loan_data.isnull().sum()

# retriving the columns which has any null values
loan_data_columns=loan_data.columns[loan_data.isnull().any()].tolist()
loan_data[loan_data_columns].isnull().sum()*100/len(loan_data)

"""tot_coll_amt, tot_cur_bal, total_rev_hi_lim have the same total missing 15% from all the data. So the three columns will be checked.


*   tot_coll_amt: Total collection amounts ever owed
*   tot_cur_bal: Total current balance of all accounts
*   total_rev_hi_lim: Total revolving high credit/credit limit
"""

# Check tot_coll_amt, tot_cur_bal, total_rev_hi_lim
total_cols = ['tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']

loan_data[total_cols].head(10)

loan_data[total_cols].sample(10)

"""*as shown in the data above, this column has a similar missing value location"""

loan_data[total_cols].describe()

def tot_cols(x):
    loan_data.boxplot(x)
    plt.show()

for cols in total_cols:
    tot_cols(cols)

"""Conclusion:

*   75% of tot_coll_amt is 0
*   the data for each row is quite different so it is not possible to fill *   in the missing value with the mean value or other value
*   total missing value 70276 = 15.07% of all data
*   so the rows of the missing value in those columns will be dropped
"""

# drop all rows that contain missing value
loan_data.dropna(subset = ['tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim'], inplace = True)

#reset index
loan_data.reset_index(drop= True, inplace = True)

"""**Pre-processing Few Continuos Variable (Data Type Transformation)**

The following variables are not possessing appropriate data types and should be modified
"""

continuous_cols = ['term', 'emp_length', 'earliest_cr_line', 'last_credit_pull_d']
loan_data[continuous_cols]

# Check the data
loan_data['term']

# Convert to numerical datatype and replace months with empty strng
loan_data['term'] = pd.to_numeric(loan_data['term'].str.replace(' months', ''))
loan_data['term']

# Displays unique values of emp_length
loan_data['emp_length'].unique()

emp_map = {
    '< 1 year' : '0',
    '1 year' : '1',
    '2 years' : '2',
    '3 years' : '3',
    '4 years' : '4',
    '5 years' : '5',
    '6 years' : '6',
    '7 years' : '7',
    '8 years' : '8',
    '9 years' : '9',
    '10+ years' : '10'
}

loan_data['emp_length'] = loan_data['emp_length'].map(emp_map).fillna('0').astype(int)
loan_data['emp_length'].unique()

# # Displays a column
loan_data['earliest_cr_line']

# Extracts the date and the time from a string variable that is in a given format
loan_data['earliest_cr_line_date'] = pd.to_datetime(loan_data['earliest_cr_line'], format = '%b-%y')

earliest_date = loan_data['earliest_cr_line_date']

earliest_date

# Assume we are now in December 2017
loan_data['mths_since_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - loan_data['earliest_cr_line_date']) / np.timedelta64(1, 'M')))
# calculate the difference between two dates in months, turn it to numeric datatype and round it.
# save the result in a new variable.

# Shows some descriptive statisics for the values of a column
loan_data['mths_since_earliest_cr_line'].describe()

# display the rows where a variable has negative value
loan_data.loc[: , ['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']][loan_data['mths_since_earliest_cr_line'] < 0]

# Change the dtype into string and replace the year 2069 etc into 1969 etc
loan_data['earliest_cr_line_date'] = loan_data['earliest_cr_line_date'].astype(str)
loan_data['earliest_cr_line_date'][loan_data['mths_since_earliest_cr_line'] < 0] = loan_data['earliest_cr_line_date'][loan_data['mths_since_earliest_cr_line'] < 0].str.replace('20','19')

# check one of the data that change from 2068 to 1968
loan_data['earliest_cr_line_date'][628]

# change dtype into datetime again
loan_data['earliest_cr_line_date'] = pd.to_datetime(loan_data['earliest_cr_line_date'])
loan_data['earliest_cr_line_date']

# check the data again to see the changes (Assume in Dec 2015)
loan_data['mths_since_earliest_cr_line_date'] = round(pd.to_numeric((pd.to_datetime('2015-12-01') - loan_data['earliest_cr_line_date']) / np.timedelta64(1, 'M')))
# Shows some descriptive statisics for the values of a column.
loan_data['mths_since_earliest_cr_line_date'].describe()

#drop column earliest_cr_line_date, mths_since_earliest_cr_line,
# and earliest_cr_line as we don't use it anymore
loan_data.drop(columns = ['earliest_cr_line_date' ,'mths_since_earliest_cr_line',
                          'earliest_cr_line'], inplace = True)

loan_data['last_credit_pull_d']

# Assume now in December 2017
# Extracts the date and the time from a string variable that is in a given format. and fill NaN data with max date
loan_data['last_credit_pull_d'] = pd.to_datetime(loan_data['last_credit_pull_d'], format = '%b-%y').fillna(pd.to_datetime("2016-01-01"))

# calculate the difference between two dates in months, turn it to numeric datatype and round it.
loan_data['mths_since_last_credit_pull_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - loan_data['last_credit_pull_d']) / np.timedelta64(1, 'M')))

# Shows some descriptive statisics for the values of a column.
loan_data['mths_since_last_credit_pull_d'].describe()

#drop column last_credit_pull_d as we don't use it anymore
loan_data.drop(columns = ['last_credit_pull_d'], inplace = True)

#Checking for missing values
loan_data.isnull().sum()

#drop column revol_util as we don't use it anymore
loan_data.drop(columns = ['revol_util'], inplace = True)

#reset index
loan_data.reset_index(drop= True, inplace = True)

#Checking for missing values
missing_value = loan_data.isnull().sum()
missing_value[missing_value>0]

"""**Explore Data**"""

def risk_percentage(x):
    ratio = (loan_data.groupby(x)['good_bad_loan'] # group by
         .value_counts(normalize=True) # calculate the ratio
         .mul(100) # multiply by 100 to be percent
         .rename('risky (%)') # rename column as percent
         .reset_index())

    sns.lineplot(data=ratio[ratio['good_bad_loan'] == 0], x=x, y='risky (%)')
    plt.title(x)
    plt.show()

print(loan_data.nunique()[loan_data.nunique() < 12].sort_values().index)

#unique columns and months date column
unq_cols = ['term', 'initial_list_status', 'verification_status',
       'home_ownership', 'acc_now_delinq', 'grade', 'inq_last_6mths',
       'collections_12_mths_ex_med', 'emp_length', 'mths_since_earliest_cr_line_date', 'mths_since_last_credit_pull_d']
for cols in unq_cols:
    risk_percentage(cols)

#Check correlation
plt.figure(figsize=(24,24))
sns.heatmap(loan_data.corr(), annot=True, annot_kws={'size':14})

# Convert categorical columns with One Hot Encoding
from sklearn.preprocessing import OneHotEncoder
cat_cols = [col for col in loan_data.select_dtypes(include='object').columns.tolist()]
onehot_cols = pd.get_dummies(loan_data[cat_cols], drop_first=True)

onehot_cols

from sklearn.preprocessing import StandardScaler

num_cols = [col for col in loan_data.columns.tolist() if col not in cat_cols + ['good_bad_loan']]
ss = StandardScaler()
std_cols = pd.DataFrame(ss.fit_transform(loan_data[num_cols]), columns=num_cols)

std_cols

"""get the final dataa"""

# combining column
final_data = pd.concat([onehot_cols, std_cols, loan_data[['good_bad_loan']]], axis=1)
final_data.head()

# separate dependant (y) and independant (X) variable
X = final_data.drop('good_bad_loan', axis = 1)
y = final_data['good_bad_loan']

#spliting data into train and test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,
                                                    random_state=42,stratify=y)

X_train.shape, X_test.shape

"""Checking for Class Imbalance in Final Dataset"""

#check if class labels are balanced
plt.title('Good (1) vs Bad (0) Loans Balance')
sns.barplot(x=final_data.good_bad_loan.value_counts().index,y=final_data.good_bad_loan.value_counts().values)

#checking  imbalance for training dataset
y_train.value_counts()

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler()
X_train_ros, y_train_ros = ros.fit_resample(X_train, y_train)

#check value counts before and after oversampling
print('Before OverSampling:\n{}'.format(y_train.value_counts()))
print('\nAfter OverSampling:\n{}'.format(y_train_ros.value_counts()))

# Import Library
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve, make_scorer, f1_score, accuracy_score

"""In this case, we  build 10 models to get which model have the best performance"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.neural_network import MLPClassifier

# Training
LR_ros= LogisticRegression(max_iter=600)
LR_ros.fit(X_train_ros, y_train_ros)

#predicting
y_pred_LR_ros = LR_ros.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_LR_ros, digits=4, target_names = target_names))

# building model
rf_ros = RandomForestClassifier(max_depth=10, n_estimators=20)
rf_ros.fit(X_train_ros, y_train_ros)

#predicting
y_pred_rf_ros = rf_ros.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_rf_ros, digits=4, target_names = target_names))

#building model
dt_ros = DecisionTreeClassifier(max_depth = 10)
dt_ros.fit(X_train_ros, y_train_ros)

#predicting
y_pred_dt_ros = dt_ros.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_dt_ros, digits=4, target_names = target_names))

#building model
knn_ros = KNeighborsClassifier(n_neighbors=20)
knn_ros.fit(X_train_ros, y_train_ros)

#predicting
y_pred_knn_ros = knn_ros.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_knn_ros, digits=4, target_names = target_names))

#building model
xgb_ros = XGBClassifier(max_depth=5)
xgb_ros.fit(X_train_ros, y_train_ros)

#predicting
y_pred_xgb_ros = xgb_ros.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_xgb_ros, digits=4, target_names = target_names))

# #building model
lgbm_ros =  LGBMClassifier(num_iterations = 600)
lgbm_ros.fit(X_train_ros, y_train_ros)

#predicting
y_pred_lgbm_ros = lgbm_ros.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_lgbm_ros, digits=4, target_names = target_names))

#building model
adb_ros = AdaBoostClassifier(n_estimators = 100)
adb_ros.fit(X_train_ros, y_train_ros)

#predicting
y_pred_adb_ros = adb_ros.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_adb_ros, digits=4, target_names = target_names))

#building model
naiveBayes_ros = GaussianNB()
naiveBayes_ros.fit(X_train_ros, y_train_ros)

#predicting
y_pred_naiveBayes_ros = naiveBayes_ros.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_naiveBayes_ros, digits=4, target_names = target_names))

#building model
QDA_ros = QuadraticDiscriminantAnalysis()
QDA_ros.fit(X_train_ros, y_train_ros)

#predicting
y_pred_QDA_ros = QDA_ros.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_QDA_ros, digits=4, target_names = target_names))

#building model
MLPC_ros = MLPClassifier(alpha=1, max_iter=1000, random_state=42)
MLPC_ros.fit(X_train_ros, y_train_ros)

#predicting
y_pred_MLPC_ros = MLPC_ros.predict(X_test)

#classification report
target_names = ['bad loan', 'good loan']
print('Classification_Report:')
print(classification_report(y_test, y_pred_MLPC_ros, digits=4, target_names = target_names))

"""# Conclusion
The best average accuracy result among all the models above is using the LGBM Classifier with an average accuracy value of 73.58% (bad loan recall = 61.82% and good loan recall = 74.95%). Although this accuracy value is still not high, this value is already quite high because of the imbalanced dataset. Recall is number of correctly predicted “positives” divided by the total number of “positives”. That means this model correctly identified 61.82% of the total bad loans and correctly identified 74.95% of the total good loans.



"""